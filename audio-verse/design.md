server runs on a 4090 machine, two models loaded in memory, for llm we have the option to call api or call locally served model with openai compatible api

1. faster-whisper large pip install faster-whisper
2. f5-tts pip install git+https://github.com/SWivid/F5-TTS.git
